{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below code is to remove trailing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('D:/606/RoadMaintenance/Datasets/filteredHPMS/FilteredHPMS2018.csv')\n",
    "\n",
    "# Strip leading and trailing spaces from all column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Strip leading and trailing spaces from all string columns\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].str.strip()\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv('D:/606/RoadMaintenance/Datasets/filteredHPMS/FilteredHPMS2018_new.csv', index=False)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to maintain equal and same number of records in HPMS data using merge and inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the first CSV file into a DataFrame\n",
    "df2 = pd.read_csv('D:/606/RoadMaintenance/HPMS/mygeodata2018/alabama_2020_pr_data.csv')\n",
    "\n",
    "# Read the second CSV file into a DataFrame\n",
    "df1 = pd.read_csv('D:/606/RoadMaintenance/HPMS/mygeodata2018/alabama_2022_pr_data.csv')\n",
    "\n",
    "# Specify the columns to match\n",
    "columns_to_match = ['ROUTE_ID', 'BEGIN_POIN']\n",
    "\n",
    "# Merge the DataFrames on the specified columns\n",
    "merged_df = pd.merge(df2, df1[columns_to_match], on=columns_to_match, how='inner')\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "merged_df.to_csv('D:/606/RoadMaintenance/HPMS/mygeodata2018/alabama_2020_pr_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPMS 2013 routeID are different when compared to other years. Below code is to change that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4028\\3420166337.py:10: DtypeWarning: Columns (16,23,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  hpms_df = pd.read_csv('D:/606/RoadMaintenance/Datasets/filteredHPMS/FilteredHPMS2013.csv')\n"
     ]
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "\n",
    "# Load the mapping file\n",
    "mapping_df = pd.read_csv('D:/606/RoadMaintenance/Datasets/Book 3(Sheet1).csv')\n",
    "\n",
    "# Create a dictionary from the mapping dataframe\n",
    "mapping_dict = pd.Series(mapping_df.New.values, index=mapping_df.Old).to_dict()\n",
    "\n",
    "# Load the HPMS file\n",
    "hpms_df = pd.read_csv('D:/606/RoadMaintenance/Datasets/filteredHPMS/FilteredHPMS2013.csv')\n",
    "\n",
    "# Replace old ROUTE_IDs with new ones using the mapping dictionary\n",
    "hpms_df['ROUTE_ID'] = hpms_df['ROUTE_ID'].replace(mapping_dict)\n",
    "\n",
    "# Save the updated dataframe back to CSV\n",
    "hpms_df.to_csv('D:/606/RoadMaintenance/Datasets/filteredHPMS/FilteredHPMS2013_1.csv', index=False)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below code helps to get data in required format i.e. 0.1 - 0.2, 0.2 - 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "path = 'D:/606/RoadMaintenance/HPMS/mygeodata2018/'\n",
    "dest = 'D:/606/RoadMaintenance/HPMS/mygeodata2018/'\n",
    "\n",
    "for year in range(2022, 2023, 1):\n",
    "\n",
    "    data = pd.read_csv(path + f'alabama_{year}_pr_data.csv')\n",
    "\n",
    "    def round_up_to_one_decimal_place(value):\n",
    "        if isinstance(value, float) and len(str(value).split('.')[1]) >= 2:\n",
    "            return math.ceil(value * 10) / 10.0\n",
    "        return value\n",
    "\n",
    "    data['BEGIN_POIN'] = data['BEGIN_POIN'].apply(round_up_to_one_decimal_place)\n",
    "    data['END_POINT'] = data['END_POINT'].apply(round_up_to_one_decimal_place)\n",
    "\n",
    "    data = data[data['BEGIN_POIN'] != data['END_POINT']]\n",
    "\n",
    "    data.to_csv(dest + f'alabama_{year}_pr_data.csv')\n",
    "    print(year)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted average for FAF from year 2018-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11728\\1258256624.py:104: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  weighted_average = CFS_Area.groupby(['dms_orig', 'dms_dest']).apply(weighted_average).reset_index()\n"
     ]
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('D:/606/RoadMaintenance/FAF/FAF4.5.1_csv_2013-2018/FAF5.6.1_2018-2023.csv')\n",
    "\n",
    "Truck_df = df[df['dms_mode'] == 1]\n",
    "\n",
    "CFS_Area = Truck_df[((Truck_df['dms_orig'] == 11) & (Truck_df['dms_dest'] == 11)) | ((Truck_df['dms_orig'] == 12) & (Truck_df['dms_dest'] == 12))]\n",
    "\n",
    "CFS_Area = CFS_Area.drop(columns=['fr_orig', 'fr_dest', 'fr_inmode', 'fr_outmode', 'sctg2', 'trade_type', 'dms_mode'])\n",
    "\n",
    "#CFS_Area = CFS_Area.rename(columns={f'tons_{year}': 'tons', f'value_{year}': 'value', f'tmiles_{year}': 'tmiles', f'curval_{year}': 'curval'})\n",
    "\n",
    "CFS_Area = CFS_Area.reset_index(drop=True)\n",
    "\n",
    "def weighted_average(group):\n",
    "    tons_2018 = group['tons_2018']\n",
    "    value_2018 = group['value_2018']\n",
    "    tmiles_2018 = group['tmiles_2018']\n",
    "    current_value_2018 = group['current_value_2018']\n",
    "    tons_2019 = group['tons_2019']\n",
    "    value_2019 = group['value_2019']\n",
    "    tmiles_2019 = group['tmiles_2019']\n",
    "    current_value_2019 = group['current_value_2019']\n",
    "    tons_2020 = group['tons_2020']\n",
    "    value_2020 = group['value_2020']\n",
    "    tmiles_2020 = group['tmiles_2020']\n",
    "    current_value_2020 = group['current_value_2020']\n",
    "    tons_2021 = group['tons_2021']\n",
    "    value_2021 = group['value_2021']\n",
    "    tmiles_2021 = group['tmiles_2021']\n",
    "    current_value_2021 = group['current_value_2021']\n",
    "    tons_2022 = group['tons_2022']\n",
    "    value_2022 = group['value_2022']\n",
    "    tmiles_2022 = group['tmiles_2022']\n",
    "    current_value_2022 = group['current_value_2022']\n",
    "    tons_2023 = group['tons_2023']\n",
    "    value_2023 = group['value_2023']\n",
    "    tmiles_2023 = group['tmiles_2023']\n",
    "    current_value_2023 = group['current_value_2023']\n",
    "\n",
    "    total_tons_2018 = tons_2018.sum()  # Total weight for each group\n",
    "    total_tons_2019 = tons_2019.sum()\n",
    "    total_tons_2020 = tons_2020.sum()\n",
    "    total_tons_2021 = tons_2021.sum()\n",
    "    total_tons_2022 = tons_2022.sum()\n",
    "    total_tons_2023 = tons_2023.sum()\n",
    "\n",
    "    # Calculate weighted averages, handling cases where total_tons is zero\n",
    "    weighted_tons_2018 = np.where(total_tons_2018 == 0, 0, (tons_2018 * tons_2018).sum() / total_tons_2018)  # Weight by tons\n",
    "    weighted_value_2018 = np.where(total_tons_2018 == 0, 0, (value_2018 * tons_2018).sum() / total_tons_2018)\n",
    "    weighted_tmiles_2018 = np.where(total_tons_2018 == 0, 0, (tmiles_2018 * tons_2018).sum() / total_tons_2018)\n",
    "    weighted_curval_2018 = np.where(total_tons_2018 == 0, 0, (current_value_2018 * tons_2018).sum() / total_tons_2018)\n",
    "    weighted_tons_2019 = np.where(total_tons_2019 == 0, 0, (tons_2019 * tons_2019).sum() / total_tons_2019)  # Weight by tons\n",
    "    weighted_value_2019 = np.where(total_tons_2019 == 0, 0, (value_2019 * tons_2019).sum() / total_tons_2019)\n",
    "    weighted_tmiles_2019 = np.where(total_tons_2019 == 0, 0, (tmiles_2019 * tons_2019).sum() / total_tons_2019)\n",
    "    weighted_curval_2019 = np.where(total_tons_2019 == 0, 0, (current_value_2019 * tons_2019).sum() / total_tons_2019)\n",
    "    weighted_tons_2020 = np.where(total_tons_2020 == 0, 0, (tons_2020 * tons_2020).sum() / total_tons_2020)  # Weight by tons\n",
    "    weighted_value_2020 = np.where(total_tons_2020 == 0, 0, (value_2020 * tons_2020).sum() / total_tons_2020)\n",
    "    weighted_tmiles_2020 = np.where(total_tons_2020 == 0, 0, (tmiles_2020 * tons_2020).sum() / total_tons_2020)\n",
    "    weighted_curval_2020 = np.where(total_tons_2020 == 0, 0, (current_value_2020 * tons_2020).sum() / total_tons_2020)\n",
    "    weighted_tons_2021 = np.where(total_tons_2021 == 0, 0, (tons_2021 * tons_2021).sum() / total_tons_2021)  # Weight by tons\n",
    "    weighted_value_2021 = np.where(total_tons_2021 == 0, 0, (value_2021 * tons_2021).sum() / total_tons_2021)\n",
    "    weighted_tmiles_2021 = np.where(total_tons_2021 == 0, 0, (tmiles_2021 * tons_2021).sum() / total_tons_2021)\n",
    "    weighted_curval_2021 = np.where(total_tons_2021 == 0, 0, (current_value_2021 * tons_2021).sum() / total_tons_2021)\n",
    "    weighted_tons_2022 = np.where(total_tons_2022 == 0, 0, (tons_2022 * tons_2022).sum() / total_tons_2022)  # Weight by tons\n",
    "    weighted_value_2022 = np.where(total_tons_2022 == 0, 0, (value_2022 * tons_2022).sum() / total_tons_2022)\n",
    "    weighted_tmiles_2022 = np.where(total_tons_2022 == 0, 0, (tmiles_2022 * tons_2022).sum() / total_tons_2022)\n",
    "    weighted_curval_2022 = np.where(total_tons_2022 == 0, 0, (current_value_2022 * tons_2022).sum() / total_tons_2022)\n",
    "    weighted_tons_2023 = np.where(total_tons_2023 == 0, 0, (tons_2023 * tons_2023).sum() / total_tons_2023)  # Weight by tons\n",
    "    weighted_value_2023 = np.where(total_tons_2023 == 0, 0, (value_2023 * tons_2023).sum() / total_tons_2023)\n",
    "    weighted_tmiles_2023 = np.where(total_tons_2023 == 0, 0, (tmiles_2023 * tons_2023).sum() / total_tons_2023)\n",
    "    weighted_curval_2023 = np.where(total_tons_2023 == 0, 0, (current_value_2023 * tons_2023).sum() / total_tons_2023)\n",
    "\n",
    "    return pd.Series({\n",
    "        'tons_2018': weighted_tons_2018,\n",
    "        'value_2018': weighted_value_2018,\n",
    "        'tmiles_2018': weighted_tmiles_2018,\n",
    "        'current_value_2018': weighted_curval_2018,\n",
    "        'tons_2019': weighted_tons_2019,\n",
    "        'value_2019': weighted_value_2019,\n",
    "        'tmiles_2019': weighted_tmiles_2019,\n",
    "        'current_value_2019': weighted_curval_2019,\n",
    "        'tons_2020': weighted_tons_2020,\n",
    "        'value_2020': weighted_value_2020,\n",
    "        'tmiles_2020': weighted_tmiles_2020,\n",
    "        'current_value_2020': weighted_curval_2020,\n",
    "        'tons_2021': weighted_tons_2021,\n",
    "        'value_2021': weighted_value_2021,\n",
    "        'tmiles_2021': weighted_tmiles_2021,\n",
    "        'current_value_2021': weighted_curval_2021,\n",
    "        'tons_2022': weighted_tons_2022,\n",
    "        'value_2022': weighted_value_2022,\n",
    "        'tmiles_2022': weighted_tmiles_2022,\n",
    "        'current_value_2022': weighted_curval_2022,\n",
    "        'tons_2023': weighted_tons_2023,\n",
    "        'value_2023': weighted_value_2023,\n",
    "        'tmiles_2023': weighted_tmiles_2023,\n",
    "        'current_value_2023': weighted_curval_2023,\n",
    "    })\n",
    "\n",
    "    # Group by origin and destination and apply the weighted average function\n",
    "weighted_average = CFS_Area.groupby(['dms_orig', 'dms_dest']).apply(weighted_average).reset_index()\n",
    "\n",
    "weighted_average.to_csv('D:/606/RoadMaintenance/Datasets/WeightedFAF/2018-2023.csv')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removes duplicate values from the HPMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to: D:/606/RoadMaintenance/HPMS/mygeodata2018/alabama_2020_pr_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"D:/606/RoadMaintenance/HPMS/mygeodata2018/alabama_2020_pr_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop duplicates based on Route_ID, Begin_point, and End_point, keeping the first occurrence\n",
    "df_cleaned = df.drop_duplicates(subset=['ROUTE_ID', 'BEGIN_POIN'], keep='first')\n",
    "\n",
    "# Save the cleaned file\n",
    "cleaned_file_path = \"D:/606/RoadMaintenance/HPMS/mygeodata2018/alabama_2020_pr_data.csv\"\n",
    "df_cleaned.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {cleaned_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converts to readable Date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp conversion complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Load your CSV file\n",
    "file_path = \"D:/606/RoadMaintenance/Datasets/FinalHPMS/FilteredHPMS2022.csv\"  # Update this if needed\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to convert milliseconds timestamp to a readable date\n",
    "def convert_timestamp(ts):\n",
    "    if pd.notna(ts) and ts > 0:  # Ensure it's not NaN and is a valid timestamp\n",
    "        return datetime.fromtimestamp(ts / 1000, tz=timezone.utc).strftime('%Y-%m-%d')\n",
    "    return ts  # Keep the original if NaN or invalid\n",
    "\n",
    "# Apply conversion only to non-null values\n",
    "df['attributes_year_last_improvement'] = df['attributes_year_last_improvement'].apply(convert_timestamp)\n",
    "\n",
    "# Save the cleaned file\n",
    "df.to_csv(\"D:/606/RoadMaintenance/Datasets/FinalHPMS/FilteredHPMS2022.csv\", index=False)\n",
    "\n",
    "print(\"Timestamp conversion complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maping of IS_improved column based year_last_improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x9c in position 12290: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Step 1: Read the first CSV file and store relevant values\u001b[39;00m\n\u001b[0;32m      4\u001b[0m first_csv \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/606/RoadMaintenance/Datasets/FinalHPMS/FilteredHPMS2023.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Path to the first CSV file\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df_first \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_csv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Extract the year from the 'attributes_last_year_improvement' column\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df_first[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_first[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattributes_year_last_improvement\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39myear\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x9c in position 12290: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the first CSV file and store relevant values\n",
    "first_csv = 'D:/606/RoadMaintenance/Datasets/FinalHPMS/FilteredHPMS2023.csv'  # Path to the first CSV file\n",
    "df_first = pd.read_csv(first_csv)\n",
    "\n",
    "# Extract the year from the 'attributes_last_year_improvement' column\n",
    "df_first['year'] = pd.to_datetime(df_first['attributes_year_last_improvement'], errors='coerce').dt.year.fillna(0).astype(int)\n",
    "\n",
    "for _, row in df_first.iterrows():\n",
    "    year = row['year']\n",
    "    route_id = row['ROUTE_ID']\n",
    "    begin_point = row['BEGIN_POIN']\n",
    "    end_point = row['END_POINT']\n",
    "\n",
    "    if year in [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2022, 2023]:\n",
    "        # Dynamically load the second CSV file based on the year\n",
    "        second_csv = f'D:/606/RoadMaintenance/Datasets/FinalHPMS/FilteredHPMS{year}.csv'  # File name pattern for the second CSV\n",
    "        try:\n",
    "            df_second = pd.read_csv(second_csv)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {second_csv} not found, skipping year {year}\")\n",
    "            continue  # Skip if the file for that year doesn't exist\n",
    "\n",
    "         # Step 4: Update the IS_IMPROVED column to 1 in the second CSV\n",
    "        df_second.loc[\n",
    "            (df_second['ROUTE_ID'] == route_id) & \n",
    "            (df_second['BEGIN_POIN'] == begin_point) & \n",
    "            (df_second['END_POINT'] == end_point),\n",
    "            'IS_IMPROVED'\n",
    "        ] = 1\n",
    "                \n",
    "        \n",
    "\n",
    "        # Step 5: Save the updated second CSV file\n",
    "        df_second.to_csv(second_csv, index=False)\n",
    "        print(f\"Updated {second_csv}\")\n",
    "    else:\n",
    "        print(f\"{year} not found\") \n",
    "\n",
    "# Print the first few rows of the updated second CSV\n",
    "print(df_first.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping FAF data to HPMS combining both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/606/RoadMaintenance/Datasets/FinalHPMS/FilteredHPMS2022.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = 'D:/606/RoadMaintenance/Datasets/FinalHPMS/'\n",
    "fafPath = 'D:/606/RoadMaintenance/Datasets/WeightedFAF/2018-2023.csv'\n",
    "\n",
    "for year in range(2022, 2023, 1):\n",
    "    df = pd.read_csv(path + f'FilteredHPMS{year}.csv')\n",
    "    faf_df = pd.read_csv(fafPath)\n",
    "\n",
    "    def updateDF(df):\n",
    "        df['tons'] = None\n",
    "        df['value'] = None\n",
    "        df['tmiles'] = None\n",
    "        df['curval'] = None\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            if row['COUNTY_COD'] in [7, 37, 115, 127, 9, 121, 43, 21, 117, 73]:\n",
    "                # Find corresponding rows in df_new where dms_origin is 11\n",
    "                match_row = faf_df[(faf_df['dms_orig'] == 11)]\n",
    "                if not match_row.empty:\n",
    "                    # Assign values from the matched row in df_new to the new columns\n",
    "                    df.at[index, 'tons'] = match_row[f'tons_{year}'].values[0]\n",
    "                    df.at[index, 'value'] = match_row[f'value_{year}'].values[0]\n",
    "                    df.at[index, 'tmiles'] = match_row[f'tmiles_{year}'].values[0]\n",
    "                    df.at[index, 'curval'] = match_row[f'current_value_{year}'].values[0]\n",
    "            else:\n",
    "                match_row = faf_df[(faf_df['dms_orig'] == 12)]\n",
    "                if not match_row.empty:\n",
    "                    # Assign values from the matched row in df_new to the new columns\n",
    "                    df.at[index, 'tons'] = match_row[f'tons_{year}'].values[0]\n",
    "                    df.at[index, 'value'] = match_row[f'value_{year}'].values[0]\n",
    "                    df.at[index, 'tmiles'] = match_row[f'tmiles_{year}'].values[0]\n",
    "                    df.at[index, 'curval'] = match_row[f'current_value_{year}'].values[0]\n",
    "        \n",
    "        return df\n",
    "\n",
    "    df = updateDF(df)\n",
    "    df.to_csv(path + f'FilteredHPMS{year}.csv')\n",
    "    print(path + f'FilteredHPMS{year}.csv')\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all HPMS data into one csv final dataset including FAF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      AADT_VN  BEGIN_POIN  COUNTY_COD  END_POINT  F_SYSTEM_V  IRI_VN  \\\n",
      "0     30520.0       317.5        43.0      317.6         1.0   138.0   \n",
      "1     30520.0       317.4        43.0      317.5         1.0    73.0   \n",
      "2     30520.0       317.3        43.0      317.4         1.0    59.0   \n",
      "3     30520.0       317.2        43.0      317.3         1.0    59.0   \n",
      "4     30520.0       317.1        43.0      317.2         1.0    59.0   \n",
      "...       ...         ...         ...        ...         ...     ...   \n",
      "7114  28092.0         0.5        97.0        0.6         1.0    36.0   \n",
      "7115  28092.0         0.4        97.0        0.5         1.0   180.0   \n",
      "7116  28092.0         0.3        97.0        0.4         1.0   102.0   \n",
      "7117  28092.0         0.2        97.0        0.3         1.0    53.0   \n",
      "7118  28092.0         0.0        97.0        0.1         1.0    70.0   \n",
      "\n",
      "      IS_IMPROVED  NHS_VN      ROUTE_ID  ROUTE_NUMB  ...  THROUGH_LA  \\\n",
      "0             NaN     1.0  IN0000650000        65.0  ...         4.0   \n",
      "1             NaN     1.0  IN0000650000        65.0  ...         4.0   \n",
      "2             NaN     1.0  IN0000650000        65.0  ...         4.0   \n",
      "3             NaN     1.0  IN0000650000        65.0  ...         4.0   \n",
      "4             NaN     1.0  IN0000650000        65.0  ...         4.0   \n",
      "...           ...     ...           ...         ...  ...         ...   \n",
      "7114          NaN     NaN  IN0000650000        65.0  ...         4.0   \n",
      "7115          NaN     NaN  IN0000650000        65.0  ...         4.0   \n",
      "7116          NaN     NaN  IN0000650000        65.0  ...         4.0   \n",
      "7117          NaN     NaN  IN0000650000        65.0  ...         4.0   \n",
      "7118          NaN     NaN  IN0000650000        65.0  ...         4.0   \n",
      "\n",
      "      TRUCK_VN  URBAN_CODE  Unnamed: 0  YEAR_RECOR       curval  \\\n",
      "0          NaN     99999.0           0        2013  1954.693232   \n",
      "1          NaN     99999.0           1        2013  1954.693232   \n",
      "2          NaN     99999.0           2        2013  1954.693232   \n",
      "3          NaN     99999.0           3        2013  1954.693232   \n",
      "4          NaN     99999.0           4        2013  1954.693232   \n",
      "...        ...         ...         ...         ...          ...   \n",
      "7114       NaN     57925.0         786        2022   566.329505   \n",
      "7115       NaN     57925.0         787        2022   566.329505   \n",
      "7116       NaN     57925.0         788        2022   566.329505   \n",
      "7117       NaN     57925.0         789        2022   566.329505   \n",
      "7118       NaN     57925.0         790        2022   566.329505   \n",
      "\n",
      "                                         geometry_paths     tmiles  \\\n",
      "0     [[[-86.90444957099999, 34.30910064200003], [-8...  81.078214   \n",
      "1     [[[-86.90556667899995, 34.30692441300005], [-8...  81.078214   \n",
      "2     [[[-86.90625466899996, 34.30559036500006], [-8...  81.078214   \n",
      "3     [[[-86.90693842599995, 34.30425456200004], [-8...  81.078214   \n",
      "4     [[[-86.90762396099996, 34.30291908300006], [-8...  81.078214   \n",
      "...                                                 ...        ...   \n",
      "7114  [[[-88.11789072099998, 30.630183491000025], [-...  17.769601   \n",
      "7115  [[[-88.11802476099996, 30.627956745000063], [-...  17.769601   \n",
      "7116  [[[-88.11833702899997, 30.627363127000024], [-...  17.769601   \n",
      "7117  [[[-88.11944581599994, 30.626288873000078], [-...  17.769601   \n",
      "7118  [[[-88.12260980999997, 30.625396570000078], [-...  17.769601   \n",
      "\n",
      "             tons        value  \n",
      "0     3558.684828  1946.066769  \n",
      "1     3558.684828  1946.066769  \n",
      "2     3558.684828  1946.066769  \n",
      "3     3558.684828  1946.066769  \n",
      "4     3558.684828  1946.066769  \n",
      "...           ...          ...  \n",
      "7114   914.285758   321.226241  \n",
      "7115   914.285758   321.226241  \n",
      "7116   914.285758   321.226241  \n",
      "7117   914.285758   321.226241  \n",
      "7118   914.285758   321.226241  \n",
      "\n",
      "[7119 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = 'D:/606/RoadMaintenance/Datasets/FinalHPMS'\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop over all files and read them into pandas dataframes\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all the dataframes, aligning by column names\n",
    "combined_df = pd.concat(dfs, axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "# Optionally, you can reorder the columns to ensure they are consistent\n",
    "combined_df = combined_df.reindex(sorted(combined_df.columns), axis=1)\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_df.to_csv('D:/606/RoadMaintenance/Datasets/FinalDataSet/FinalFilteredCombined.csv', index=False)\n",
    "\n",
    "# Display the combined dataframe (optional)\n",
    "print(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
