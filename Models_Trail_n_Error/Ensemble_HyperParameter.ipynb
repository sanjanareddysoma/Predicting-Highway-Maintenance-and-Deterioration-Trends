{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/606/RoadMaintenance/Main/filtered_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Records from 2013 to 2020 → Training data (df_recent)\n",
    "df_train = df[df['YEAR_RECOR'].isin([2013, 2014, 2015, 2016, 2017, 2018, 2019])]\n",
    "\n",
    "# Records from 2021 and 2022 → Testing data (df_past)\n",
    "df_test = df[df['YEAR_RECOR'].isin([2020, 2022])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest Regressor ---\n",
      "MSE: 354.3478, R^2: 0.6027\n",
      "Train MSE: 91.9327, Train R^2: 0.8953\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "target = 'IRI_VN'\n",
    "features = ['YEAR_RECOR', 'AADT_VN', 'curval', 'tmiles', 'tons', 'value', 'IS_IMPROVED', 'SPEED_LIMI', 'BEGIN_POIN', 'END_POINT', 'THROUGH_LA', 'SECTION_NUM']\n",
    "\n",
    "# Create training and testing sets\n",
    "X_train, y_train = df_train[features], df_train[target]\n",
    "X_test, y_test = df_test[features], df_test[target]\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate_model(predictions, y_test):\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    return mse, r2\n",
    "\n",
    "# --- Random Forest Regressor Section ---\n",
    "print(\"\\n--- Random Forest Regressor ---\")\n",
    "random_forest_regressor = RandomForestRegressor(random_state=42, n_estimators=80, max_features='sqrt', bootstrap=True, max_samples=0.5, max_depth=25)\n",
    "random_forest_regressor.fit(X_train, y_train)\n",
    "rf_predictions = random_forest_regressor.predict(X_test)\n",
    "rf_train_predictions = random_forest_regressor.predict(X_train)\n",
    "rf_mse, rf_r2 = evaluate_model(rf_predictions, y_test)\n",
    "rf_train_mse, rf_train_r2 = evaluate_model(rf_train_predictions, y_train)\n",
    "print(f\"MSE: {rf_mse:.4f}, R^2: {rf_r2:.4f}\")\n",
    "print(f\"Train MSE: {rf_train_mse:.4f}, Train R^2: {rf_train_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Gradient Boosting Regressor ---\n",
      "MSE: 418.0456, R^2: 0.5313\n"
     ]
    }
   ],
   "source": [
    "# --- Gradient Boosting Regressor Section ---\n",
    "print(\"\\n--- Gradient Boosting Regressor ---\")\n",
    "gradient_boosting_regressor = GradientBoostingRegressor(random_state=42)\n",
    "gradient_boosting_regressor.fit(X_train, y_train)\n",
    "gb_predictions = gradient_boosting_regressor.predict(X_test)\n",
    "gb_mse, gb_r2 = evaluate_model(gb_predictions, y_test)\n",
    "print(f\"MSE: {gb_mse:.4f}, R^2: {gb_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'max_depth=6, \\n                                 min_child_weight=1, \\n                                 gamma=0, \\n                                 colsample_bytree=1, \\n                                 reg_alpha=0, \\n                                 reg_lambda=1,\\n                                 subsample=0.5'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''max_depth=6, \n",
    "                                 min_child_weight=1, \n",
    "                                 gamma=0, \n",
    "                                 colsample_bytree=1, \n",
    "                                 reg_alpha=0, \n",
    "                                 reg_lambda=1,\n",
    "                                 subsample=0.5'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- XGBoost Regressor ---\n",
      "MSE: 440.1356, R^2: 0.5065\n"
     ]
    }
   ],
   "source": [
    "# --- XGBoost Regressor Section ---\n",
    "print(\"\\n--- XGBoost Regressor ---\")\n",
    "xgboost_regressor = XGBRegressor(random_state=42, \n",
    "                                 n_estimators=10, \n",
    "                                 learning_rate=0.3,)\n",
    "xgboost_regressor.fit(X_train, y_train)\n",
    "xgboost_predictions = xgboost_regressor.predict(X_test)\n",
    "xgboost_mse, xgboost_r2 = evaluate_model(xgboost_predictions, y_test)\n",
    "print(f\"MSE: {xgboost_mse:.4f}, R^2: {xgboost_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Voting Regressor ---\n",
      "MSE: 383.6152, R^2: 0.5699\n"
     ]
    }
   ],
   "source": [
    "# --- Voting Regressor Section ---\n",
    "print(\"\\n--- Voting Regressor ---\")\n",
    "voting_regressor = VotingRegressor(estimators=[('rf', random_forest_regressor), ('gb', gradient_boosting_regressor), ('xgb', xgboost_regressor)])\n",
    "voting_regressor.fit(X_train, y_train)\n",
    "voting_predictions = voting_regressor.predict(X_test)\n",
    "voting_mse, voting_r2 = evaluate_model(voting_predictions, y_test)\n",
    "print(f\"MSE: {voting_mse:.4f}, R^2: {voting_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble Model Evaluations:\n",
      "              Models         MSE       R^2\n",
      "0      Random Forest  354.347782  0.602715\n",
      "1  Gradient Boosting  418.045624  0.531299\n",
      "2            XGBoost  440.135559  0.506532\n",
      "3   Voting Regressor  383.615193  0.569902\n"
     ]
    }
   ],
   "source": [
    "# --- Store Results ---\n",
    "model_evaluations = {\n",
    "    \"Models\": [\"Random Forest\", \"Gradient Boosting\", \"XGBoost\", \"Voting Regressor\"],\n",
    "    \"MSE\": [rf_mse, gb_mse, xgboost_mse, voting_mse],\n",
    "    \"R^2\": [rf_r2, gb_r2, xgboost_r2, voting_r2],\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(model_evaluations)\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"\\nEnsemble Model Evaluations:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nBias-Variance Tradeoff Analysis:/n\n",
      "              Models   Train MSE    Test MSE  Train R^2  Test R^2\n",
      "0      Random Forest   91.932687  354.347782   0.895264  0.602715\n",
      "1  Gradient Boosting  333.379044  418.045624   0.620191  0.531299\n",
      "2   Voting Regressor  191.219559  383.615193   0.782149  0.569902\n",
      "3            XGBoost  238.397507  440.135559   0.728401  0.506532\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models on training data (to check bias)\n",
    "rf_train_predictions = random_forest_regressor.predict(X_train)\n",
    "gb_train_predictions = gradient_boosting_regressor.predict(X_train)\n",
    "voting_train_predictions = voting_regressor.predict(X_train)\n",
    "xgboost_train_predictions = xgboost_regressor.predict(X_train)\n",
    "\n",
    "# Compute MSE & R^2 for training data\n",
    "rf_train_mse, rf_train_r2 = evaluate_model(rf_train_predictions, y_train)\n",
    "gb_train_mse, gb_train_r2 = evaluate_model(gb_train_predictions, y_train)\n",
    "voting_train_mse, voting_train_r2 = evaluate_model(voting_train_predictions, y_train)\n",
    "xgboost_train_mse, xgboost_train_r2 = evaluate_model(xgboost_train_predictions, y_train)\n",
    "\n",
    "# Compute bias-variance tradeoff\n",
    "bias_variance_df = pd.DataFrame({\n",
    "    \"Models\": [\"Random Forest\", \"Gradient Boosting\", \"Voting Regressor\",\"XGBoost\"],\n",
    "    \"Train MSE\": [rf_train_mse, gb_train_mse, voting_train_mse, xgboost_train_mse],\n",
    "    \"Test MSE\": [rf_mse, gb_mse, voting_mse, xgboost_mse],\n",
    "    \"Train R^2\": [rf_train_r2, gb_train_r2, voting_train_r2, xgboost_train_r2],\n",
    "    \"Test R^2\": [rf_r2, gb_r2, voting_r2, xgboost_r2],\n",
    "})\n",
    "\n",
    "print(\"/nBias-Variance Tradeoff Analysis:/n\")\n",
    "print(bias_variance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_rf = {\n",
    "'n_estimators': [50, 60, 70, 80, 90, 100],\n",
    "'max_depth': [None, 5, 10, 15, 20, 25],\n",
    "'min_samples_split': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.1],\n",
    "'min_samples_leaf': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.1],\n",
    "'max_features': ['sqrt', 'log2', None, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "param_grid_gb = {\n",
    "\n",
    "'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 150, 200],\n",
    "'learning_rate': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0],\n",
    "'max_depth': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\n",
    "'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "'min_samples_split': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.1],\n",
    "'min_samples_leaf': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.1]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "param_grid_xgb = {\n",
    "\n",
    "'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 150, 200],\n",
    "'learning_rate': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0],\n",
    "'max_depth': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\n",
    "'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "'colsample_bytree': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "'min_child_weight': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\n",
    "'gamma':[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\n",
    "'reg_alpha':[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\n",
    "'reg_lambda': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25]\n",
    "\n",
    "}\n",
    "\n",
    "# # Initialize the models\n",
    "# rf = RandomForestRegressor(random_state=42)\n",
    "# gb = GradientBoostingRegressor(random_state=42)\n",
    "# xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, n_jobs=-1, verbose=2, scoring= ['neg_mean_squared_error','r2'], refit='r2')\n",
    "# grid_search_gb = GridSearchCV(estimator=gb, param_grid=param_grid_gb, cv=5, n_jobs=-1, verbose=2, scoring=['neg_mean_squared_error', 'r2'], refit='r2')\n",
    "# #grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=5, n_jobs=-1, verbose=2, scoring=['neg_mean_squared_error', 'r2'], refit='r2')\n",
    "\n",
    "# # Fit the models\n",
    "# grid_search_rf.fit(X_train, y_train)\n",
    "# grid_search_gb.fit(X_train, y_train)\n",
    "# #grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best models\n",
    "# best_rf = grid_search_rf.best_estimator_\n",
    "# best_gb = grid_search_gb.best_estimator_\n",
    "# #best_xgb = grid_search_xgb.best_estimator_\n",
    "\n",
    "# # Make predictions using the best models\n",
    "# rf_predictions = best_rf.predict(X_test)\n",
    "# gb_predictions = best_gb.predict(X_test)\n",
    "# #xgboost_predictions = best_xgb.predict(X_test)\n",
    "\n",
    "# # Evaluate the models\n",
    "# rf_mse, rf_r2 = evaluate_model(rf_predictions, y_test)\n",
    "# gb_mse, gb_r2 = evaluate_model(gb_predictions, y_test)\n",
    "# #xgboost_mse, xgboost_r2 = evaluate_model(xgboost_predictions, y_test)\n",
    "\n",
    "# # Print results\n",
    "# print(f\"Best Random Forest: {grid_search_rf.best_params_}\")\n",
    "# print(f\"Best Gradient Boosting: {grid_search_gb.best_params_}\")\n",
    "# #print(f\"Best XGBoost: {grid_search_xgb.best_params_}\")\n",
    "# print(f\"Random Forest MSE: {rf_mse}, R²: {rf_r2}\")\n",
    "# print(f\"Gradient Boosting MSE: {gb_mse}, R²: {gb_r2}\")\n",
    "# #print(f\"XGBoost MSE: {xgboost_mse}, R²: {xgboost_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nBias-Variance Tradeoff Analysis:/n\n",
      "              Models   Train MSE    Test MSE  Train R^2  Test R^2\n",
      "0      Random Forest   91.932687  354.347782   0.895264  0.602715\n",
      "1  Gradient Boosting  333.379044  418.045624   0.620191  0.531299\n",
      "2   Voting Regressor  191.219559  383.615193   0.782149  0.569902\n",
      "3            XGBoost  238.397507  440.135559   0.728401  0.506532\n"
     ]
    }
   ],
   "source": [
    "#Bias-Variance Tradeoff\n",
    "# Evaluate models on training data (to check bias)\n",
    "rf_train_predictions = random_forest_regressor.predict(X_train)\n",
    "gb_train_predictions = gradient_boosting_regressor.predict(X_train)\n",
    "voting_train_predictions = voting_regressor.predict(X_train)\n",
    "xgboost_train_predictions = xgboost_regressor.predict(X_train)\n",
    "\n",
    "# Compute MSE & R^2 for training data\n",
    "rf_train_mse, rf_train_r2 = evaluate_model(rf_train_predictions, y_train)\n",
    "gb_train_mse, gb_train_r2 = evaluate_model(gb_train_predictions, y_train)\n",
    "voting_train_mse, voting_train_r2 = evaluate_model(voting_train_predictions, y_train)\n",
    "xgboost_train_mse, xgboost_train_r2 = evaluate_model(xgboost_train_predictions, y_train)\n",
    "\n",
    "# Compute bias-variance tradeoff\n",
    "bias_variance_df = pd.DataFrame({\n",
    "    \"Models\": [\"Random Forest\", \"Gradient Boosting\", \"Voting Regressor\",\"XGBoost\"],\n",
    "    \"Train MSE\": [rf_train_mse, gb_train_mse, voting_train_mse, xgboost_train_mse],\n",
    "    \"Test MSE\": [rf_mse, gb_mse, voting_mse, xgboost_mse],\n",
    "    \"Train R^2\": [rf_train_r2, gb_train_r2, voting_train_r2, xgboost_train_r2],\n",
    "    \"Test R^2\": [rf_r2, gb_r2, voting_r2, xgboost_r2],\n",
    "})\n",
    "\n",
    "print(\"/nBias-Variance Tradeoff Analysis:/n\")\n",
    "print(bias_variance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'max_depth': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\\n'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\\n'colsample_bytree': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\\n'min_child_weight': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\\n'gamma':[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\\n'reg_alpha':[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\\n'reg_lambda': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25]\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''max_depth': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\n",
    "'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "'colsample_bytree': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "'min_child_weight': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\n",
    "'gamma':[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\n",
    "'reg_alpha':[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\n",
    "'reg_lambda': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_xgb = {\n",
    "\n",
    "'n_estimators': [50, 80, 100],\n",
    "'learning_rate': [0.005, 0.01, 0.04],\n",
    "'max_depth': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\n",
    "'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "'colsample_bytree': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "'min_child_weight': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\n",
    "'gamma':[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\n",
    "'reg_alpha':[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25],\n",
    "'reg_lambda': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n",
      "Best XGBoost: {'subsample': 1.0, 'reg_lambda': 7, 'reg_alpha': 2, 'n_estimators': 100, 'min_child_weight': 20, 'max_depth': 1, 'learning_rate': 0.005, 'gamma': 5, 'colsample_bytree': 0.1}\n",
      "XGBoost MSE: 907.0047607421875, R²: -0.01690816879272461\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Initialize the models\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "#random_search_rf = RandomizedSearchCV(estimator=rf, param_distributions=param_grid_rf, n_iter=162, cv=5, n_jobs=-1, verbose=2, random_state=42, scoring=['neg_mean_squared_error', 'r2'], refit='r2')\n",
    "#random_search_gb = RandomizedSearchCV(estimator=gb, param_distributions=param_grid_gb, n_iter=162, cv=5, n_jobs=-1, verbose=2, random_state=42, scoring=['neg_mean_squared_error', 'r2'], refit='r2')\n",
    "random_search_xgb = RandomizedSearchCV(estimator=xgb, param_distributions=param_grid_xgb, n_iter=1000, cv=5, n_jobs=-1, verbose=2, random_state=42, scoring=['neg_mean_squared_error', 'r2'], refit='r2')\n",
    "\n",
    "# Fit the models\n",
    "#random_search_rf.fit(X_train, y_train)\n",
    "#random_search_gb.fit(X_train, y_train)\n",
    "random_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best models\n",
    "#best_rf = random_search_rf.best_estimator_\n",
    "#best_gb = random_search_gb.best_estimator_\n",
    "best_xgb = random_search_xgb.best_estimator_\n",
    "\n",
    "# Make predictions using the best models\n",
    "#rf_predictions = best_rf.predict(X_test)\n",
    "#gb_predictions = best_gb.predict(X_test)\n",
    "xgboost_predictions = best_xgb.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "#rf_mse, rf_r2 = evaluate_model(rf_predictions, y_test)\n",
    "#gb_mse, gb_r2 = evaluate_model(gb_predictions, y_test)\n",
    "xgboost_mse, xgboost_r2 = evaluate_model(xgboost_predictions, y_test)\n",
    "\n",
    "# Print results\n",
    "#print(f\"Best Random Forest: {random_search_rf.best_params_}\")\n",
    "#print(f\"Best Gradient Boosting: {random_search_gb.best_params_}\")\n",
    "print(f\"Best XGBoost: {random_search_xgb.best_params_}\")\n",
    "#print(f\"Random Forest MSE: {rf_mse}, R²: {rf_r2}\")\n",
    "#print(f\"Gradient Boosting MSE: {gb_mse}, R²: {gb_r2}\")\n",
    "print(f\"XGBoost MSE: {xgboost_mse}, R²: {xgboost_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
